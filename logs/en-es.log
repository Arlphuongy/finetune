root@fbf8b3063406:/workspace/finetune# python3 finetune.py
Downloading data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18.1M/18.1M [00:03<00:00, 5.01MB/s]
Downloading data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3.52M/3.52M [00:00<00:00, 7.94MB/s]
Generating train split: 100268 examples [00:00, 141850.00 examples/s]
Generating validation split: 25068 examples [00:00, 430591.99 examples/s]
tokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44.0/44.0 [00:00<00:00, 170kB/s]
config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.47k/1.47k [00:00<00:00, 6.78MB/s]
source.spm: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 802k/802k [00:00<00:00, 54.7MB/s]
target.spm: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 826k/826k [00:00<00:00, 4.00MB/s]
vocab.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.59M/1.59M [00:00<00:00, 3.09MB/s]
Map:   0%|                                                                                                                                   | 0/100268 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3866: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100268/100268 [00:14<00:00, 7071.57 examples/s]
Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25068/25068 [00:02<00:00, 8739.57 examples/s]
pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 312M/312M [00:01<00:00, 221MB/s]
/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
generation_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 293/293 [00:00<00:00, 1.32MB/s]
Traceback (most recent call last):
  File "/workspace/finetune/finetune.py", line 64, in <module>
    args = Seq2SeqTrainingArguments(
  File "<string>", line 126, in __init__
  File "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py", line 1378, in __post_init__
    raise ValueError(
ValueError: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: steps
- Save strategy: epoch
root@fbf8b3063406:/workspace/finetune# git pull
remote: Enumerating objects: 5, done.
remote: Counting objects: 100% (5/5), done.
remote: Compressing objects: 100% (1/1), done.
remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0
Unpacking objects: 100% (3/3), 287 bytes | 11.00 KiB/s, done.
From https://github.com/ThaiLe1220/finetune
   741c314..48dae33  main       -> origin/main
Updating 741c314..48dae33
Fast-forward
 finetune.py | 5 ++---
 1 file changed, 2 insertions(+), 3 deletions(-)
root@fbf8b3063406:/workspace/finetune# python3 finetune.py
/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
{'loss': 1.5304, 'learning_rate': 1.966499162479062e-06, 'epoch': 0.08}
{'loss': 1.4396, 'learning_rate': 1.9329983249581237e-06, 'epoch': 0.17}
{'loss': 1.4275, 'learning_rate': 1.8994974874371858e-06, 'epoch': 0.25}
{'loss': 1.4246, 'learning_rate': 1.8659966499162478e-06, 'epoch': 0.34}
{'loss': 1.4143, 'learning_rate': 1.8324958123953097e-06, 'epoch': 0.42}
{'loss': 1.3948, 'learning_rate': 1.7989949748743718e-06, 'epoch': 0.5}
{'loss': 1.405, 'learning_rate': 1.7654941373534337e-06, 'epoch': 0.59}
{'loss': 1.4126, 'learning_rate': 1.7319932998324958e-06, 'epoch': 0.67}
{'loss': 1.3814, 'learning_rate': 1.6984924623115577e-06, 'epoch': 0.75}
{'loss': 1.378, 'learning_rate': 1.6649916247906198e-06, 'epoch': 0.84}
{'loss': 1.3905, 'learning_rate': 1.6314907872696816e-06, 'epoch': 0.92}
{'eval_loss': 1.08742094039917, 'eval_bleu': 38.9538, 'eval_gen_len': 9.9521, 'eval_runtime': 354.9488, 'eval_samples_per_second': 70.624, 'eval_steps_per_second': 0.842, 'epo
ch': 1.0}
 20%|██████████████████████████▊                                                                                                           | 1194/5970 [07:53<07:56, 10.03it/s]
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}
{'loss': 1.4021, 'learning_rate': 1.5979899497487437e-06, 'epoch': 1.01}
{'loss': 1.3661, 'learning_rate': 1.5644891122278056e-06, 'epoch': 1.09}
{'loss': 1.3825, 'learning_rate': 1.5309882747068677e-06, 'epoch': 1.17}
{'loss': 1.3788, 'learning_rate': 1.4974874371859296e-06, 'epoch': 1.26}
{'loss': 1.3561, 'learning_rate': 1.4639865996649917e-06, 'epoch': 1.34}
{'loss': 1.3666, 'learning_rate': 1.4304857621440535e-06, 'epoch': 1.42}
{'loss': 1.3791, 'learning_rate': 1.3969849246231156e-06, 'epoch': 1.51}
{'loss': 1.374, 'learning_rate': 1.3634840871021775e-06, 'epoch': 1.59}
{'loss': 1.3565, 'learning_rate': 1.3299832495812396e-06, 'epoch': 1.68}
{'loss': 1.3607, 'learning_rate': 1.2964824120603015e-06, 'epoch': 1.76}
{'loss': 1.3575, 'learning_rate': 1.2629815745393636e-06, 'epoch': 1.84}
{'loss': 1.3646, 'learning_rate': 1.2294807370184255e-06, 'epoch': 1.93}
{'eval_loss': 1.076377034187317, 'eval_bleu': 39.3255, 'eval_gen_len': 9.9265, 'eval_runtime': 354.5269, 'eval_samples_per_second': 70.708, 'eval_steps_per_second': 0.843, 'ep
och': 2.0}
 40%|█████████████████████████████████████████████████████▌                                                                                | 2388/5970 [15:47<06:06,  9.78it/s]
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}
{'loss': 1.3484, 'learning_rate': 1.1959798994974875e-06, 'epoch': 2.01}
{'loss': 1.3539, 'learning_rate': 1.1624790619765494e-06, 'epoch': 2.09}
{'loss': 1.3507, 'learning_rate': 1.1289782244556115e-06, 'epoch': 2.18}
{'loss': 1.3345, 'learning_rate': 1.0954773869346734e-06, 'epoch': 2.26}
{'loss': 1.3573, 'learning_rate': 1.0619765494137355e-06, 'epoch': 2.35}
{'loss': 1.3349, 'learning_rate': 1.0284757118927974e-06, 'epoch': 2.43}
{'loss': 1.3472, 'learning_rate': 9.949748743718592e-07, 'epoch': 2.51}
{'loss': 1.3436, 'learning_rate': 9.614740368509211e-07, 'epoch': 2.6}
{'loss': 1.363, 'learning_rate': 9.279731993299832e-07, 'epoch': 2.68}
{'loss': 1.3467, 'learning_rate': 8.944723618090452e-07, 'epoch': 2.76}
{'loss': 1.3521, 'learning_rate': 8.609715242881072e-07, 'epoch': 2.85}
{'loss': 1.3568, 'learning_rate': 8.274706867671692e-07, 'epoch': 2.93}
{'eval_loss': 1.070444107055664, 'eval_bleu': 39.4439, 'eval_gen_len': 9.9228, 'eval_runtime': 353.6576, 'eval_samples_per_second': 70.882, 'eval_steps_per_second': 0.845, 'ep
och': 3.0}
 60%|████████████████████████████████████████████████████████████████████████████████▍                                                     | 3582/5970 [23:41<04:05,  9.73it/s]
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}