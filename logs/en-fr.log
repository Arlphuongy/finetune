root@fbf8b3063406:/workspace/finetune# python3 finetune.py
Downloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████| 18.7M/18.7M [00:04<00:00, 4.66MB/s]
Downloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████| 3.64M/3.64M [00:00<00:00, 4.86MB/s]
Generating train split: 104070 examples [00:00, 144252.10 examples/s]
Generating validation split: 26018 examples [00:00, 380346.17 examples/s]
Downloading builder script: 100%|███████████████████████████████████████████████████████████████████████████| 8.15k/8.15k [00:00<00:00, 4.21MB/s]
tokenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████| 42.0/42.0 [00:00<00:00, 219kB/s]
config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 1.42k/1.42k [00:00<00:00, 8.72MB/s]
source.spm: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 778k/778k [00:00<00:00, 29.7MB/s]
target.spm: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 802k/802k [00:00<00:00, 1.94MB/s]
vocab.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 1.34M/1.34M [00:00<00:00, 12.3MB/s]
/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3866: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map: 100%|██████████████████████████████████████████████████████████████████████████████████████| 104070/104070 [00:14<00:00, 7082.31 examples/s]
Map: 100%|████████████████████████████████████████████████████████████████████████████████████████| 26018/26018 [00:02<00:00, 8976.75 examples/s]
pytorch_model.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████| 301M/301M [00:01<00:00, 260MB/s]
/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
generation_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████| 293/293 [00:00<00:00, 3.03MB/s]
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
{'loss': 1.6088, 'learning_rate': 1.987707437000615e-05, 'epoch': 0.03}
{'loss': 1.587, 'learning_rate': 1.9754148740012292e-05, 'epoch': 0.06}
{'loss': 1.5619, 'learning_rate': 1.963122311001844e-05, 'epoch': 0.09}
{'loss': 1.5979, 'learning_rate': 1.950829748002459e-05, 'epoch': 0.12}
{'loss': 1.5658, 'learning_rate': 1.9385371850030732e-05, 'epoch': 0.15}
{'loss': 1.5652, 'learning_rate': 1.926244622003688e-05, 'epoch': 0.18}
{'loss': 1.5724, 'learning_rate': 1.9139520590043026e-05, 'epoch': 0.22}
{'loss': 1.5508, 'learning_rate': 1.9016594960049172e-05, 'epoch': 0.25}
{'loss': 1.5284, 'learning_rate': 1.8893669330055316e-05, 'epoch': 0.28}
{'loss': 1.5826, 'learning_rate': 1.8770743700061466e-05, 'epoch': 0.31}
  6%|██████▍                                                                                                  | 500/8135 [00:39<10:22, 12.27it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}
{'loss': 1.561, 'learning_rate': 1.8647818070067613e-05, 'epoch': 0.34}
{'loss': 1.5481, 'learning_rate': 1.8524892440073756e-05, 'epoch': 0.37}
{'loss': 1.553, 'learning_rate': 1.8401966810079903e-05, 'epoch': 0.4}
{'loss': 1.5325, 'learning_rate': 1.827904118008605e-05, 'epoch': 0.43}
{'loss': 1.5371, 'learning_rate': 1.8156115550092193e-05, 'epoch': 0.46}
{'loss': 1.5488, 'learning_rate': 1.8033189920098343e-05, 'epoch': 0.49}
{'loss': 1.5333, 'learning_rate': 1.791026429010449e-05, 'epoch': 0.52}
{'loss': 1.55, 'learning_rate': 1.7787338660110634e-05, 'epoch': 0.55}
{'loss': 1.5269, 'learning_rate': 1.766441303011678e-05, 'epoch': 0.58}
{'loss': 1.5335, 'learning_rate': 1.7541487400122927e-05, 'epoch': 0.61}
 12%|████████████▊                                                                                           | 1000/8135 [01:21<09:39, 12.32it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}
{'loss': 1.5338, 'learning_rate': 1.7418561770129074e-05, 'epoch': 0.65}
{'loss': 1.5311, 'learning_rate': 1.729563614013522e-05, 'epoch': 0.68}
{'loss': 1.5234, 'learning_rate': 1.7172710510141367e-05, 'epoch': 0.71}
{'loss': 1.5388, 'learning_rate': 1.7049784880147514e-05, 'epoch': 0.74}
{'loss': 1.5176, 'learning_rate': 1.6926859250153658e-05, 'epoch': 0.77}
{'loss': 1.5077, 'learning_rate': 1.6803933620159804e-05, 'epoch': 0.8}
{'loss': 1.4988, 'learning_rate': 1.668100799016595e-05, 'epoch': 0.83}
{'loss': 1.5054, 'learning_rate': 1.6558082360172098e-05, 'epoch': 0.86}
{'loss': 1.5239, 'learning_rate': 1.6435156730178245e-05, 'epoch': 0.89}
{'loss': 1.5172, 'learning_rate': 1.631223110018439e-05, 'epoch': 0.92}
 18%|███████████████████▏                                                                                    | 1500/8135 [02:04<08:55, 12.38it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}
{'loss': 1.5273, 'learning_rate': 1.6189305470190535e-05, 'epoch': 0.95}
{'loss': 1.5229, 'learning_rate': 1.606637984019668e-05, 'epoch': 0.98}
{'eval_loss': 1.2405380010604858, 'eval_bleu': 29.474, 'eval_gen_len': 11.2287, 'eval_runtime': 392.1567, 'eval_samples_per_second': 66.346, 'eva
l_steps_per_second': 1.038, 'epoch': 1.0}
{'loss': 1.4642, 'learning_rate': 1.594345421020283e-05, 'epoch': 1.01}
{'loss': 1.4258, 'learning_rate': 1.5820528580208975e-05, 'epoch': 1.04}
{'loss': 1.4232, 'learning_rate': 1.5697602950215122e-05, 'epoch': 1.08}
{'loss': 1.429, 'learning_rate': 1.557467732022127e-05, 'epoch': 1.11}
{'loss': 1.4681, 'learning_rate': 1.5451751690227416e-05, 'epoch': 1.14}
{'loss': 1.4414, 'learning_rate': 1.532882606023356e-05, 'epoch': 1.17}
{'loss': 1.4174, 'learning_rate': 1.5205900430239706e-05, 'epoch': 1.2}
{'loss': 1.4531, 'learning_rate': 1.5082974800245852e-05, 'epoch': 1.23}
 25%|█████████████████████████▌                                                                              | 2000/8135 [09:47<16:31,  6.19it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}
{'loss': 1.4321, 'learning_rate': 1.4960049170251998e-05, 'epoch': 1.26}
{'loss': 1.4094, 'learning_rate': 1.4837123540258144e-05, 'epoch': 1.29}
{'loss': 1.4147, 'learning_rate': 1.4714197910264291e-05, 'epoch': 1.32}
{'loss': 1.4212, 'learning_rate': 1.4591272280270436e-05, 'epoch': 1.35}
{'loss': 1.428, 'learning_rate': 1.4468346650276583e-05, 'epoch': 1.38}
{'loss': 1.452, 'learning_rate': 1.434542102028273e-05, 'epoch': 1.41}
{'loss': 1.4323, 'learning_rate': 1.4222495390288877e-05, 'epoch': 1.44}
{'loss': 1.4179, 'learning_rate': 1.4099569760295022e-05, 'epoch': 1.48}
{'loss': 1.4238, 'learning_rate': 1.3976644130301168e-05, 'epoch': 1.51}
{'loss': 1.4439, 'learning_rate': 1.3853718500307315e-05, 'epoch': 1.54}
 31%|███████████████████████████████▉                                                                        | 2500/8135 [10:31<07:30, 12.50it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}
{'loss': 1.4304, 'learning_rate': 1.373079287031346e-05, 'epoch': 1.57}
{'loss': 1.4239, 'learning_rate': 1.3607867240319607e-05, 'epoch': 1.6}
{'loss': 1.3913, 'learning_rate': 1.3484941610325754e-05, 'epoch': 1.63}
{'loss': 1.4237, 'learning_rate': 1.3362015980331899e-05, 'epoch': 1.66}
{'loss': 1.4236, 'learning_rate': 1.3239090350338046e-05, 'epoch': 1.69}
{'loss': 1.4075, 'learning_rate': 1.3116164720344192e-05, 'epoch': 1.72}
{'loss': 1.4229, 'learning_rate': 1.2993239090350338e-05, 'epoch': 1.75}
{'loss': 1.4229, 'learning_rate': 1.2870313460356484e-05, 'epoch': 1.78}
{'loss': 1.4289, 'learning_rate': 1.2747387830362631e-05, 'epoch': 1.81}
{'loss': 1.4239, 'learning_rate': 1.262446220036878e-05, 'epoch': 1.84}
 37%|██████████████████████████████████████▎                                                                 | 3000/8135 [11:14<06:59, 12.25it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}
{'loss': 1.408, 'learning_rate': 1.2501536570374923e-05, 'epoch': 1.87}
{'loss': 1.4317, 'learning_rate': 1.237861094038107e-05, 'epoch': 1.91}
{'loss': 1.4432, 'learning_rate': 1.2255685310387218e-05, 'epoch': 1.94}
{'loss': 1.4305, 'learning_rate': 1.2132759680393362e-05, 'epoch': 1.97}
{'loss': 1.4343, 'learning_rate': 1.2009834050399508e-05, 'epoch': 2.0}
{'eval_loss': 1.229009985923767, 'eval_bleu': 29.8165, 'eval_gen_len': 11.1951, 'eval_runtime': 394.6227, 'eval_samples_per_second': 65.931, 'eva
l_steps_per_second': 1.031, 'epoch': 2.0}
{'loss': 1.3514, 'learning_rate': 1.1886908420405657e-05, 'epoch': 2.03}
{'loss': 1.3541, 'learning_rate': 1.17639827904118e-05, 'epoch': 2.06}
{'loss': 1.33, 'learning_rate': 1.1641057160417947e-05, 'epoch': 2.09}
{'loss': 1.3449, 'learning_rate': 1.1518131530424096e-05, 'epoch': 2.12}
{'loss': 1.3654, 'learning_rate': 1.1395205900430239e-05, 'epoch': 2.15}
 43%|████████████████████████████████████████████▋                                                           | 3500/8135 [18:31<06:27, 11.96it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}
{'loss': 1.3694, 'learning_rate': 1.1272280270436386e-05, 'epoch': 2.18}
{'loss': 1.3768, 'learning_rate': 1.1149354640442534e-05, 'epoch': 2.21}
{'loss': 1.369, 'learning_rate': 1.1026429010448681e-05, 'epoch': 2.24}
{'loss': 1.3603, 'learning_rate': 1.0903503380454824e-05, 'epoch': 2.27}
{'loss': 1.3638, 'learning_rate': 1.0780577750460973e-05, 'epoch': 2.3}
{'loss': 1.3574, 'learning_rate': 1.065765212046712e-05, 'epoch': 2.34}
{'loss': 1.3158, 'learning_rate': 1.0534726490473263e-05, 'epoch': 2.37}
{'loss': 1.3681, 'learning_rate': 1.0411800860479411e-05, 'epoch': 2.4}
{'loss': 1.3342, 'learning_rate': 1.0288875230485558e-05, 'epoch': 2.43}
{'loss': 1.3499, 'learning_rate': 1.0165949600491702e-05, 'epoch': 2.46}
 49%|███████████████████████████████████████████████████▏                                                    | 4000/8135 [19:13<05:24, 12.75it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}
{'loss': 1.3447, 'learning_rate': 1.004302397049785e-05, 'epoch': 2.49}
{'loss': 1.3654, 'learning_rate': 9.920098340503997e-06, 'epoch': 2.52}
{'loss': 1.3744, 'learning_rate': 9.797172710510142e-06, 'epoch': 2.55}
{'loss': 1.3495, 'learning_rate': 9.674247080516289e-06, 'epoch': 2.58}
{'loss': 1.3615, 'learning_rate': 9.551321450522436e-06, 'epoch': 2.61}
{'loss': 1.379, 'learning_rate': 9.42839582052858e-06, 'epoch': 2.64}
{'loss': 1.3466, 'learning_rate': 9.305470190534727e-06, 'epoch': 2.67}
{'loss': 1.3506, 'learning_rate': 9.182544560540874e-06, 'epoch': 2.7}
{'loss': 1.3692, 'learning_rate': 9.05961893054702e-06, 'epoch': 2.74}
{'loss': 1.367, 'learning_rate': 8.936693300553166e-06, 'epoch': 2.77}
 55%|█████████████████████████████████████████████████████████▌                                              | 4500/8135 [19:56<04:44, 12.76it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}
{'loss': 1.3556, 'learning_rate': 8.813767670559313e-06, 'epoch': 2.8}
{'loss': 1.3636, 'learning_rate': 8.690842040565458e-06, 'epoch': 2.83}
{'loss': 1.3388, 'learning_rate': 8.567916410571605e-06, 'epoch': 2.86}
{'loss': 1.3935, 'learning_rate': 8.444990780577751e-06, 'epoch': 2.89}
{'loss': 1.3642, 'learning_rate': 8.322065150583898e-06, 'epoch': 2.92}
{'loss': 1.365, 'learning_rate': 8.199139520590043e-06, 'epoch': 2.95}
{'loss': 1.3581, 'learning_rate': 8.07621389059619e-06, 'epoch': 2.98}
{'eval_loss': 1.2249584197998047, 'eval_bleu': 29.9826, 'eval_gen_len': 11.1645, 'eval_runtime': 406.9517, 'eval_samples_per_second': 63.934, 'eval_steps_per_se
cond': 1.0, 'epoch': 3.0}
{'loss': 1.3368, 'learning_rate': 7.953288260602337e-06, 'epoch': 3.01}
{'loss': 1.3305, 'learning_rate': 7.830362630608482e-06, 'epoch': 3.04}
{'loss': 1.3021, 'learning_rate': 7.707437000614629e-06, 'epoch': 3.07}
 61%|█████████████████████████████████████████████████████████████████████████▏                                             | 5000/8135 [27:27<04:05, 12.78it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}
{'loss': 1.2928, 'learning_rate': 7.5845113706207755e-06, 'epoch': 3.1}
{'loss': 1.3161, 'learning_rate': 7.461585740626921e-06, 'epoch': 3.13}
{'loss': 1.2839, 'learning_rate': 7.338660110633068e-06, 'epoch': 3.17}
{'loss': 1.286, 'learning_rate': 7.215734480639214e-06, 'epoch': 3.2}
{'loss': 1.3101, 'learning_rate': 7.092808850645359e-06, 'epoch': 3.23}
{'loss': 1.2915, 'learning_rate': 6.969883220651507e-06, 'epoch': 3.26}
{'loss': 1.2991, 'learning_rate': 6.846957590657653e-06, 'epoch': 3.29}
{'loss': 1.301, 'learning_rate': 6.7240319606638e-06, 'epoch': 3.32}
{'loss': 1.3067, 'learning_rate': 6.6011063306699456e-06, 'epoch': 3.35}
{'loss': 1.3122, 'learning_rate': 6.4781807006760915e-06, 'epoch': 3.38}
 68%|████████████████████████████████████████████████████████████████████████████████▍                                      | 5500/8135 [28:09<03:25, 12.85it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}
{'loss': 1.3124, 'learning_rate': 6.355255070682238e-06, 'epoch': 3.41}
{'loss': 1.3282, 'learning_rate': 6.232329440688384e-06, 'epoch': 3.44}
{'loss': 1.3292, 'learning_rate': 6.109403810694531e-06, 'epoch': 3.47}
{'loss': 1.3347, 'learning_rate': 5.986478180700677e-06, 'epoch': 3.5}
{'loss': 1.3027, 'learning_rate': 5.863552550706823e-06, 'epoch': 3.53}
{'loss': 1.3297, 'learning_rate': 5.74062692071297e-06, 'epoch': 3.56}
{'loss': 1.3309, 'learning_rate': 5.6177012907191156e-06, 'epoch': 3.6}
{'loss': 1.2945, 'learning_rate': 5.4947756607252615e-06, 'epoch': 3.63}
{'loss': 1.3181, 'learning_rate': 5.371850030731408e-06, 'epoch': 3.66}
{'loss': 1.3193, 'learning_rate': 5.248924400737554e-06, 'epoch': 3.69}
 74%|███████████████████████████████████████████████████████████████████████████████████████▊                               | 6000/8135 [28:52<02:49, 12.60it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}
{'loss': 1.2944, 'learning_rate': 5.125998770743701e-06, 'epoch': 3.72}
{'loss': 1.3343, 'learning_rate': 5.003073140749847e-06, 'epoch': 3.75}
{'loss': 1.3197, 'learning_rate': 4.880147510755993e-06, 'epoch': 3.78}
{'loss': 1.3287, 'learning_rate': 4.75722188076214e-06, 'epoch': 3.81}
{'loss': 1.283, 'learning_rate': 4.6342962507682856e-06, 'epoch': 3.84}
{'loss': 1.3159, 'learning_rate': 4.5113706207744315e-06, 'epoch': 3.87}
{'loss': 1.3003, 'learning_rate': 4.388444990780578e-06, 'epoch': 3.9}
{'loss': 1.3161, 'learning_rate': 4.265519360786724e-06, 'epoch': 3.93}
{'loss': 1.3112, 'learning_rate': 4.14259373079287e-06, 'epoch': 3.96}
{'loss': 1.3224, 'learning_rate': 4.019668100799017e-06, 'epoch': 4.0}
 80%|███████████████████████████████████████████████████████████████████████████████████████████████                        | 6500/8135 [29:35<02:06, 12.97it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}
{'eval_loss': 1.2249640226364136, 'eval_bleu': 29.9669, 'eval_gen_len': 11.164, 'eval_runtime': 398.2719, 'eval_samples_per_second': 65.327, 'eval_steps_per_sec
ond': 1.022, 'epoch': 4.0}
{'loss': 1.3004, 'learning_rate': 3.896742470805163e-06, 'epoch': 4.03}
{'loss': 1.2686, 'learning_rate': 3.7738168408113096e-06, 'epoch': 4.06}
{'loss': 1.2995, 'learning_rate': 3.650891210817456e-06, 'epoch': 4.09}
{'loss': 1.2821, 'learning_rate': 3.527965580823602e-06, 'epoch': 4.12}
{'loss': 1.2735, 'learning_rate': 3.4050399508297483e-06, 'epoch': 4.15}
{'loss': 1.273, 'learning_rate': 3.2821143208358946e-06, 'epoch': 4.18}
{'loss': 1.2972, 'learning_rate': 3.159188690842041e-06, 'epoch': 4.21}
{'loss': 1.2705, 'learning_rate': 3.036263060848187e-06, 'epoch': 4.24}
{'loss': 1.2938, 'learning_rate': 2.9133374308543333e-06, 'epoch': 4.27}
{'loss': 1.2918, 'learning_rate': 2.7904118008604796e-06, 'epoch': 4.3}
 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▍                | 7000/8135 [36:57<01:30, 12.52it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}
{'loss': 1.2893, 'learning_rate': 2.667486170866626e-06, 'epoch': 4.33}
{'loss': 1.2842, 'learning_rate': 2.544560540872772e-06, 'epoch': 4.36}
{'loss': 1.2781, 'learning_rate': 2.4216349108789183e-06, 'epoch': 4.39}
{'loss': 1.2877, 'learning_rate': 2.2987092808850646e-06, 'epoch': 4.43}
{'loss': 1.2735, 'learning_rate': 2.175783650891211e-06, 'epoch': 4.46}
{'loss': 1.3086, 'learning_rate': 2.0528580208973573e-06, 'epoch': 4.49}
{'loss': 1.2904, 'learning_rate': 1.9299323909035037e-06, 'epoch': 4.52}
{'loss': 1.2783, 'learning_rate': 1.8070067609096498e-06, 'epoch': 4.55}
{'loss': 1.2507, 'learning_rate': 1.6840811309157962e-06, 'epoch': 4.58}
{'loss': 1.2611, 'learning_rate': 1.5611555009219423e-06, 'epoch': 4.61}
 92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋         | 7500/8135 [37:39<00:50, 12.46it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}
{'loss': 1.2725, 'learning_rate': 1.4382298709280887e-06, 'epoch': 4.64}
{'loss': 1.2984, 'learning_rate': 1.3153042409342348e-06, 'epoch': 4.67}
{'loss': 1.3202, 'learning_rate': 1.1923786109403812e-06, 'epoch': 4.7}
{'loss': 1.2887, 'learning_rate': 1.0694529809465275e-06, 'epoch': 4.73}
{'loss': 1.2806, 'learning_rate': 9.465273509526737e-07, 'epoch': 4.76}
{'loss': 1.2783, 'learning_rate': 8.236017209588199e-07, 'epoch': 4.79}
{'loss': 1.2669, 'learning_rate': 7.006760909649663e-07, 'epoch': 4.82}
{'loss': 1.2493, 'learning_rate': 5.777504609711126e-07, 'epoch': 4.86}
{'loss': 1.282, 'learning_rate': 4.5482483097725875e-07, 'epoch': 4.89}
{'loss': 1.2988, 'learning_rate': 3.318992009834051e-07, 'epoch': 4.92}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 8000/8135 [38:21<00:11, 12.25it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}
{'loss': 1.3013, 'learning_rate': 2.0897357098955133e-07, 'epoch': 4.95}
{'loss': 1.286, 'learning_rate': 8.604794099569762e-08, 'epoch': 4.98}
{'eval_loss': 1.2251585721969604, 'eval_bleu': 29.9618, 'eval_gen_len': 11.1642, 'eval_runtime': 398.4935, 'eval_samples_per_second': 65.291, 'eval_steps_per_se
cond': 1.021, 'epoch': 5.0}
{'train_runtime': 2714.091, 'train_samples_per_second': 191.722, 'train_steps_per_second': 2.997, 'train_loss': 1.3844882995567955, 'epoch': 5.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8135/8135 [45:14<00:00,  3.00it/s]
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}
opus-mt-en-fr-finetuned/training_args.bin
opus-mt-en-fr-finetuned/target.spm
opus-mt-en-fr-finetuned/source.spm
opus-mt-en-fr-finetuned/vocab.json
opus-mt-en-fr-finetuned/special_tokens_map.json
opus-mt-en-fr-finetuned/tokenizer_config.json
opus-mt-en-fr-finetuned/model.safetensors
opus-mt-en-fr-finetuned/generation_config.json
opus-mt-en-fr-finetuned/config.json
opus-mt-en-fr-finetuned/checkpoint-8000/trainer_state.json
opus-mt-en-fr-finetuned/checkpoint-8000/rng_state.pth
opus-mt-en-fr-finetuned/checkpoint-8000/scheduler.pt
opus-mt-en-fr-finetuned/checkpoint-8000/optimizer.pt
opus-mt-en-fr-finetuned/checkpoint-8000/training_args.bin
opus-mt-en-fr-finetuned/checkpoint-8000/target.spm
opus-mt-en-fr-finetuned/checkpoint-8000/source.spm
opus-mt-en-fr-finetuned/checkpoint-8000/vocab.json
opus-mt-en-fr-finetuned/checkpoint-8000/special_tokens_map.json
opus-mt-en-fr-finetuned/checkpoint-8000/tokenizer_config.json
opus-mt-en-fr-finetuned/checkpoint-8000/model.safetensors
opus-mt-en-fr-finetuned/checkpoint-8000/generation_config.json
opus-mt-en-fr-finetuned/checkpoint-8000/config.json
opus-mt-en-fr-finetuned/checkpoint-7500/trainer_state.json
opus-mt-en-fr-finetuned/checkpoint-7500/rng_state.pth
opus-mt-en-fr-finetuned/checkpoint-7500/scheduler.pt
opus-mt-en-fr-finetuned/checkpoint-7500/optimizer.pt
opus-mt-en-fr-finetuned/checkpoint-7500/training_args.bin
opus-mt-en-fr-finetuned/checkpoint-7500/target.spm
opus-mt-en-fr-finetuned/checkpoint-7500/source.spm
opus-mt-en-fr-finetuned/checkpoint-7500/vocab.json
opus-mt-en-fr-finetuned/checkpoint-7500/special_tokens_map.json
opus-mt-en-fr-finetuned/checkpoint-7500/tokenizer_config.json
opus-mt-en-fr-finetuned/checkpoint-7500/model.safetensors
opus-mt-en-fr-finetuned/checkpoint-7500/generation_config.json
opus-mt-en-fr-finetuned/checkpoint-7500/config.json
opus-mt-en-fr-finetuned/checkpoint-7000/trainer_state.json
opus-mt-en-fr-finetuned/checkpoint-7000/rng_state.pth
opus-mt-en-fr-finetuned/checkpoint-7000/scheduler.pt
opus-mt-en-fr-finetuned/checkpoint-7000/optimizer.pt
opus-mt-en-fr-finetuned/checkpoint-7000/training_args.bin
opus-mt-en-fr-finetuned/checkpoint-7000/target.spm
opus-mt-en-fr-finetuned/checkpoint-7000/source.spm
opus-mt-en-fr-finetuned/checkpoint-7000/vocab.json
opus-mt-en-fr-finetuned/checkpoint-7000/special_tokens_map.json
opus-mt-en-fr-finetuned/checkpoint-7000/tokenizer_config.json
opus-mt-en-fr-finetuned/checkpoint-7000/model.safetensors
opus-mt-en-fr-finetuned/checkpoint-7000/generation_config.json
opus-mt-en-fr-finetuned/checkpoint-7000/config.json
["Je m'appelle Sarah, je vis à Londres avec ma famille et j'adore ça."]